---
description: 'CMRNet: Camera to LiDAR-Map Registration'
---

# \[ITSC 2021] CMRNet

## Abstract

In this paper we present CMRNet, a realtime approach based on a Convolutional Neural Network (CNN) to localize an RGB image of a scene in a map built from LiDAR data. Our network is not trained in the working area, i.e., CMRNet does not learn the map. Instead it learns to match an image to the map. We validate our approach on the KITTI dataset, processing each frame independently without any tracking procedure. CMRNet achieves 0.27m and 1.07◦ median localization accuracy on the sequence 00 of the odometry dataset, starting from a rough pose estimate displaced up to 3.5m and 17◦ . To the best of our knowledge this is the first CNN-based approach that learns to match images from a monocular camera to a given, preexisting 3D LiDAR-map.

## Introduction

Localization approaches that utilize the same sensor for mapping and localization usually achieve good performances, as the map of the scene is matched to the same kind of data generated by the on-board sensor. However, their application is hampered by the need for a preliminary mapping of the working area, which represents a relevant issue in terms of effort both for building the maps as well as for their maintenance.

In contrast, we here propose a novel method for registering an image from an on-board monocular RGB camera to a LiDAR-map of the area. This allows for the exploitation of the forthcoming market of LiDAR-maps embedded into HD maps using only a cheap camera-based sensor suite on the vehicle.

In particular, we propose CMRNet, a CNN-based approach that achieves camera localization with sub-meter accuracy, basing on a rough initial pose estimate. The maps and images used for localization are not necessarily those used during the training of the network. To the best of our knowledge, this is the first work to tackle the localization problem without a localized CNN, i.e., a CNN trained in the working area. CMRNet does not learn the map, instead, it learns to match images to the LiDAR-map. Extensive experimental evaluations performed on the KITTI datasets show the feasibility of our approach.

## Proposed Approach

In this work, we aim at localizing a camera from a single image in a 3D LiDAR-map of an urban environment.

<figure><img src="../../../.gitbook/assets/image (951).png" alt=""><figcaption></figcaption></figure>

算法流程如图1所示，根据相机初始位姿$$H_{init}$$将地图点云投影到虚拟的相机平面上，得到合成的深度图（称为Lidar image）。将Lidar image和RGB图像输入CMRNet，得到两视角之间的相对位姿$$H_{out}$$​，由$$H_{out}$$​和初始位姿$$H_{init}$$​得到相机的6DoF位姿。

刚体变换矩阵是一个4x4的矩阵：

<figure><img src="../../../.gitbook/assets/image (938).png" alt=""><figcaption></figcaption></figure>

其中，R是3x3旋转矩阵，T是3x1的平移矩阵。由于旋转矩阵用9个元素来表示3个自由度，我们用在3-sphere($$S^3$$)流形上的四元数来表示旋转。所以网络的输出为平移向量$$T\in R^3$$​，旋转四元数$$q \in S^3$$。简化起见，将网络的输出定义为$$H_{out}$$​，表示我们将T和q转换成了刚体变换矩阵。

### Lidar Image Generation

根据初始位姿$$H_{init}$$​得到Lidar image。

#### Map Projection

将所有3D点投影到位于$$H_{init}$$​处的虚拟相机平面，即对每个3D点P计算其相机坐标p：

<figure><img src="../../../.gitbook/assets/image (993).png" alt=""><figcaption></figcaption></figure>

然后用z-buffer方法取判断相同投影线上点的可见性。我们只对$$H_{init}$$​附近的子区域进行了投影，并忽略了投影到虚拟相机平面外的点。

#### Occlusion Filtering

点云投影可能会产生不真实的深度图。为此，作者采用了点云遮挡估计滤波器。对于每个点$$P_i$$，关于到相机的投影线构建一个锥体，这一​锥体不与其他任何点相交。如果锥体的孔径大于预设阈值Th，该点被标记为“可见”。从技术角度讲，对于Lidar image中每个深度不为0的点$$p_j$$​，计算从其对应3D点$$P_j$$​到该点的单位向量$$\overrightarrow{v}$$​，对于任何投影在​$$p_j$$邻域(K x K)内的3D点$$P_i$$，计算向量$$\overrightarrow{c}=\frac{P_i-P_j}{||P_i-P_j||}$$和两向量之间的夹角$$\theta=arccos(\overrightarrow{v}\cdot \overrightarrow{c})$$.用该夹角去判断$$P_j$$的可见度。遮挡的点在Lidar image中被置为0.​

<figure><img src="../../../.gitbook/assets/image (904).png" alt=""><figcaption></figcaption></figure>

### Network Architecture

以PWC-Net作为baseline，并作了一些改动：由于输入是RGB图像和深度图像，所以解耦了feature encoder，两个输入之间参数不共享；去掉上采样层，在第一个cost volume层后接全连接层用于位姿回归。损失函数包括平移损失函数和旋转损失函数：

<figure><img src="../../../.gitbook/assets/image (929).png" alt=""><figcaption></figcaption></figure>

其中平移损失函数用smooth L1 loss；旋转损失函数如下：

<figure><img src="../../../.gitbook/assets/image (913).png" alt=""><figcaption></figcaption></figure>

### Iterative Refinement

We propose an iterative refinement approach. In particular, we trained different CNNs by considering descending error ranges for both the translation and rotation components of the initial pose. Once a LiDAR-image is obtained for a given camera pose, both the camera and the LiDAR-image are processed, starting from the CNN that has been trained with the largest error range. Then, a new projection of the map points is performed, and the process is repeated using a CNN trained with a reduced error range. Repeating this operation n times is possible to improve the accuracy of the final localization. The improvement is achieved thanks to the increasing overlap between the scene observed from the camera and the scene projected in the $$n^{th}$$ LiDAR-image.

{% hint style="info" %}
这里居然是对不同误差范围（迭代优化中的不同阶段）训练不同的模型，是因为模型难以泛化到不同误差范围么？感觉有待改进。
{% endhint %}

### Training Details

We implemented CMRNet using the PyTorch library, and a slightly modified version of the official PWC-Net implementation. Regarding the activation function, we used a leaky RELU (REctified Linear Unit) with a negative slope of 0.1 as non-linearity. Finally, CMRNet was trained from scratch for 300 epochs using the ADAM optimizer with default parameters, a batch size of 24 and a learning rate of $$1e^{-4}$$ on a single NVidia GTX 1080ti.

## Experimental Results

We tested the localization accuracy of our method on the KITTI odometry dataset. Specifically, we used the sequences from 03 to 09 for training (11697 frames) and the sequence 00 for validating (4541 frames). We used a LiDAR-based SLAM system to obtain consistent trajectories. The resulting poses are used to generate a down-sampled map with a resolution of 0.1m.

<figure><img src="../../../.gitbook/assets/image (964).png" alt=""><figcaption></figcaption></figure>

<figure><img src="../../../.gitbook/assets/image (918).png" alt=""><figcaption></figcaption></figure>

<figure><img src="../../../.gitbook/assets/image (931).png" alt=""><figcaption></figcaption></figure>

<figure><img src="../../../.gitbook/assets/image (962).png" alt=""><figcaption></figcaption></figure>
